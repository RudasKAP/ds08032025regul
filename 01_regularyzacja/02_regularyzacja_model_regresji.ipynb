{"cells":[{"cell_type":"markdown","metadata":{"id":"zMo8cxbTCcle"},"source":["# Regularyzacja w modelu regresji - porównanie regresji grzbietowej i regresji Lasso"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgmLVsgCCcln"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import random\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from matplotlib.pylab import rcParams\n","rcParams['figure.figsize'] = 12, 10\n","\n","from sklearn.linear_model import LinearRegression\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import PolynomialFeatures"]},{"cell_type":"markdown","metadata":{"id":"z9-93hQKCclq"},"source":["**Regularyzacja grzbietowa** i **Lasso** są technikami, które są wykorzystywane do budowania **oszczędnych modeli**, w rozumieniu obecności zbyt dużej liczby predyktorów. \n","Przez dużą liczbę predyktorów rozumiemy:\n","\n","- *duża liczba predyktorów* to taka, która prowadzi do **przeuczenia modelu** (ang. *overfitting*) -- nawet tak niewielka liczba jak 10 zmiennych może prowadzić do przeuczenia,\n","    \n","- *duża liczba predyktorów* to taka, która może prowadzić do problemów z **wydajnością obliczeniową** -- przy obecnych możliwościach komputerów, taka sytuacja może mieć miejsce przy występowaniu milionów lub miliardów cech."]},{"cell_type":"markdown","metadata":{"id":"1t9pE-OECcls"},"source":["Techniki regularyzacyjne działają poprzez \n","- karanie wielkości współczynników cech, \n","- minimalizowanie błędu między przewidywanymi a rzeczywistymi obserwacjami."]},{"cell_type":"markdown","metadata":{"id":"JsPjX354Cclt"},"source":["## Dlaczego karamy za wielkość współczynników?"]},{"cell_type":"markdown","metadata":{"id":"pIq_SG2nCclu"},"source":["Rozważmy następujący przykład celem zrozumienia wpływu złożoności modelu na wielkość współczynników.\n","\n","W tym celu dopasujmy krzywą regresji do krzywej sinusoidalnej (od 0° do 360°) z dodanym szumem."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MM2_RDpyCclu"},"outputs":[],"source":["# np.random.seed(123) "]},{"cell_type":"code","source":["# #generacja danych\n","# x = np.array([i*np.pi/180 for i in range(0,360,1)])\n","# X = pd.DataFrame(x)\n","# y = np.sin(x)+np.random.normal(0,0.2,len(x))\n","# plt.plot(x,y,'.',color = 'black')"],"metadata":{"id":"9rqBkfWM7MVE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zGbJfTobCcly"},"outputs":[],"source":["# #lambda to taka prosta funkcja, bierze dowolną liczbę argumentów, ale może mieć w sobie tylko jedno wyrażenie\n","# rss_fun  = lambda y, y_pred: sum((y_pred-y)**2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RZY2fOOBCcly"},"outputs":[],"source":["# #funkcja tworzy model regresji wielomianowej bez regularyzacji, dopasowuje model do danych i rysuje wykres dla dopasowanych wartości w modelu\n","# #zwraca RSS, wyraz wolny i resztę współczynników \n","# def linear_regression(X, y, power, models_to_plot):\n","#     reg = make_pipeline(PolynomialFeatures(power),\n","#                         StandardScaler(),\n","#                         LinearRegression())\n","#     reg.fit(X, y)\n","#     y_pred = reg.predict(X)\n","    \n","#     if power in models_to_plot:\n","#         plt.subplot(models_to_plot[power])\n","#         plt.tight_layout()\n","#         plt.plot(x, y_pred)\n","#         plt.plot(x, y, '.')\n","#         plt.title('Plot for power: %d' % power)\n","    \n","#     ret = [rss_fun(y, y_pred)]\n","#     ret.extend([reg.named_steps['linearregression'].intercept_])\n","#     ret.extend(reg.named_steps['linearregression'].coef_[1:])\n","#     return ret"]},{"cell_type":"code","source":["# #rysujemy wykres regresji wielomianowej bez regularyzacji, dla różnych potęg\n","# col = ['RSS', 'Intercept'] + ['coef_x_%d' % i for i in range(1, 16)]\n","# ind = ['model_pow_%d' % i for i in range(1, 16)]\n","# coef_matrix_simple = pd.DataFrame(index=ind, columns=col)\n","\n","# models_to_plot = {1:231, 3:232, 6:233, 9:234, 12:235, 15:236}\n","\n","# for i in range(1, 16):\n","#     coef_matrix_simple.iloc[i-1, 0:i+2] = linear_regression(X, y, power=i, models_to_plot=models_to_plot)"],"metadata":{"id":"IwoBoC077Xrw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pd.options.display.float_format = '{:,.2g}'.format\n","# coef_matrix_simple"],"metadata":{"id":"P5JZgz3d7s3I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5C1Ugg21Ccl0"},"source":["**Podsumowanie**:\n","\n","- wielkość współczynników regresji rośnie eksponencjalnie wraz ze wzrostem złożoności modelu,\n","- wielkość współczynnika regresji wpływa na istotność zmiennej odpowiadającej temu współczynnikowi w oszacowaniu wielkość zmiennej odpowiedzi, ale gdy wielkość współczynnik jest zbyt duża, algorytm modeluje skomplikowane relacje w celu oszacowania wyników, co często kończy się zbytnim dopasowaniem do danych,"]},{"cell_type":"markdown","metadata":{"id":"9zRr3AcfCcl1"},"source":["# Regularyzacja grzbietowa (ang. *rigde regression*)"]},{"cell_type":"markdown","metadata":{"id":"D6kUW89ECcl2"},"source":["Metoda najmniejszych kwadratów z regularyzacją $l2$, minimalizuje **funkcję kryterialną**:\n","\n","$$||y - Xb||^2_2 + \\alpha \\cdot ||b||^2_2,$$\n","\n","gdzie dla dowolnego wektora $n$-wymiarowego $a = (a_1, a_2, \\ldots, a_n)$ zachodzi: $||a||_2 = \\sqrt{\\sum_{i=1}^n a_i^2}$."]},{"cell_type":"markdown","metadata":{"id":"rs2lUs8MCcl2"},"source":["$\\alpha$ - siła regularyzacja, $\\alpha > 0$ \n","\n","* gdy $\\alpha = 0$ -- problem uprasza się do zwykłej regresji\n","* gdy $\\alpha = +\\infty$ -- współczynnik są równe zeru"]},{"cell_type":"markdown","metadata":{"id":"KN43FaERCcl2"},"source":["## Zadanie 1\n","Napisz funkcję, która dla dowolnego zbioru ($X$ i $y$) oraz stopnia wielomianu dopasuje model regresji wielomianowej z regularyzacją Ridge z danym parametrem $\\alpha$. Ponadto, funkcja narysuje wykres rozproszenia i dopasowaną funkcję regresji dla $k$ danych wartości parametru $\\alpha$ przy ustalonym stopniu wielomianu (parametr \n","`models_to_plot`).\n","\n","Następnie wyznacz ramkę danych `coef_matrix_ridge` dla ustalonego stopnia wielomianu (np. 15), której wiersze dla ustalonej będą zawierały: wartość RSS oraz kolejne wartości współczynników regresji dla różnych parametrów $\\alpha$, np. lista `alpha_ridge`.\n","\n","Sprawdź jak zmieniają się wartości współczynników regresji z regularyzacją grzbietową wraz ze zmianą parametru $\\alpha$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XsBUISHKCcl3"},"outputs":[],"source":["# #analogiczna funkcja co wcześniej, ale teraz dla Ridge\n","# from sklearn.linear_model import Ridge\n","\n","# def ridge_regression(X, y, alpha, power, models_to_plot={}):\n","#     ridgereg = make_pipeline(PolynomialFeatures(power),\n","#                         StandardScaler(),\n","#                         Ridge(alpha = alpha))\n","#     ridgereg.fit(X, y)\n","#     y_pred = ridgereg.predict(X)\n","    \n","#     if alpha in models_to_plot:\n","#         plt.subplot(models_to_plot[alpha])\n","#         plt.tight_layout()\n","#         plt.plot(x, y_pred)\n","#         plt.plot(x, y, '.')\n","#         plt.title('Plot for alpha: %.3g' % alpha)\n","    \n","#     ret = [rss_fun(y, y_pred)]\n","#     ret.extend([ridgereg.named_steps['ridge'].intercept_])\n","#     ret.extend(ridgereg.named_steps['ridge'].coef_[1:])\n","#     return ret"]},{"cell_type":"code","source":["# #rysujemy wykres regresji wielomianowej z regularyzacją ridge,dla różnych alph dla potęgi 15\n","# alpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n","\n","# col = ['RSS','Intercept'] + ['coef_x_%d'%i for i in range(1, 16)]\n","# ind = ['alpha_%.2g' % alpha_ridge[i] for i in range(0, len(alpha_ridge))]\n","# coef_matrix_ridge = pd.DataFrame(index=ind, columns=col)\n","\n","# models_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}\n","\n","\n","# power = 15\n","# for i in range(10):\n","#     coef_matrix_ridge.iloc[i,] = ridge_regression(X, y, alpha_ridge[i], power, models_to_plot)"],"metadata":{"id":"hIDedOqa9S4U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pd.options.display.float_format = '{:,.2g}'.format\n","# coef_matrix_ridge"],"metadata":{"id":"hsdzdA2N9Vz0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pmrznEmACcl4"},"source":["**Podsumowanie**:\n","\n","- wielkość RSS (suma kwadratów błędów) rośnie wraz ze wzrostem wartości $\\alpha$, wraz z redukcją złożoności modelu,\n","- $\\alpha = 1e-15$ daje istotną redukcję wielkości współczynników regresji,\n","- wyższe wartości $\\alpha$ prowadzą do niedouczenia modelu (gwałtowny wzrost RSS dla $\\alpha > 1$.\n","- wiele współczynników jest bardzo małych, ale nie równych zeru."]},{"cell_type":"markdown","metadata":{"id":"HHKFGuEiCcl4"},"source":["# Regularyzacja Lasso (ang. *Lasso regression*)"]},{"cell_type":"markdown","metadata":{"id":"vrEjRoNzCcl5"},"source":["LASSO - Least Absolute Shrinkage and Selection Operator"]},{"cell_type":"markdown","metadata":{"id":"NFGw-0xxCcl5"},"source":["Metoda najmniejszych kwadratów z regularyzacją $l1$, minimalizuje **funkcję kryterialną**:\n","\n","$$||y - Xb||^2_2 + \\alpha \\cdot ||b||_1,$$\n","\n","gdzie dla dowolnego wektora $n$-wymiarowego $a = (a_1, a_2, \\ldots, a_n)$ zachodzi: $||a||_1 = \\sum_{i=1}^n |a_i|$."]},{"cell_type":"markdown","metadata":{"id":"0Xr_7uS-Ccl5"},"source":["## Zadanie 2\n","Napisz funkcję, która dla dowolnego zbioru ($X$ i $y$) oraz stopnia wielomianu dopasuje model regresji wielomianowej z regularyzacją Lasso z danym parametrem $\\alpha$. Ponadto, funkcja narysuje wykres rozproszenia i dopasowaną funkcję regresji dla $k$ danych wartości parametru $\\alpha$ przy ustalonym stopniu wielomianu (parametr \n","`models_to_plot`).\n","\n","Następnie wyznacz ramkę danych `coef_matrix_ridge` dla ustalonego stopnia wielomianu (np. 15), której wiersze dla ustalonej będą zawierały: wartość RSS oraz kolejne wartości współczynników regresji dla różnych parametrów $\\alpha$, np. lista `alpha_ridge`.\n","\n","Sprawdź jak zmieniają się wartości współczynników regresji z regularyzacją Lasso wraz ze zmianą parametru $\\alpha$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wUkLNiqCcl5"},"outputs":[],"source":["# #definiujemy funkcję, która dopasowuje model lasso dla regresji wielomianowej i tworzy wykresy dla pewnych alph, zwraca współczynniki i RSS\n","\n","# from sklearn.linear_model import Lasso\n","\n","# def lasso_regression(X, y, alpha, power, models_to_plot={}):\n","#     lassoreg = make_pipeline(PolynomialFeatures(power),\n","#                         StandardScaler(),\n","#                         Lasso(alpha = alpha,max_iter = 1000000))\n","#     lassoreg.fit(X, y)\n","#     y_pred = lassoreg.predict(X)\n","    \n","#     if alpha in models_to_plot:\n","#         plt.subplot(models_to_plot[alpha])\n","#         plt.tight_layout()\n","#         plt.plot(x, y_pred)\n","#         plt.plot(x, y, '.')\n","#         plt.title('Plot for alpha: %.3g' % alpha)\n","    \n","#     ret = [rss_fun(y, y_pred)]\n","#     ret.extend([lassoreg.named_steps['lasso'].intercept_])\n","#     ret.extend(lassoreg.named_steps['lasso'].coef_[1:])\n","#     return ret    "]},{"cell_type":"code","source":["# #rysujemy wykres regresji wielomianowej z regularyzacją lasso, dla różnych alph dla potęgi 15\n","# alpha_lasso = [1e-15, 1e-10, 1e-8,1e-5, 1e-4, 1e-3,1e-2, 1, 5, 10]\n","\n","# col = ['RSS','Intercept'] + ['coef_x_%d'%i for i in range(1, 16)]\n","# ind = ['alpha_%.2g' % alpha_lasso[i] for i in range(0, len(alpha_lasso))]\n","# coef_matrix_lasso = pd.DataFrame(index=ind, columns=col)\n","\n","# models_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}\n","\n","\n","# power = 15\n","# for i in range(10):\n","#     coef_matrix_lasso.iloc[i,] = lasso_regression(X, y, alpha_lasso[i], power, models_to_plot)"],"metadata":{"id":"LdBzqVj69bHo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pd.options.display.float_format = '{:,.2g}'.format\n","# coef_matrix_lasso"],"metadata":{"id":"V2olveo99euI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1fAd4MazCcl6"},"source":["**Podsumowanie**:\n","\n","- wielkość RSS (suma kwadratów błędów) rośnie wraz ze wzrostem wartości 𝛼, wraz z redukcją złożoności modelu,\n","- dla tych samym wartości $\\alpha$, wielkość współczynników regresji z regularyzacją Lasso jest mniejsza niż wartości odpowiadających współczynników w regresji z regularyzacją grzbietową,\n","- dla tych samych wartości $\\alpha$ regresji z regularyzacją Lasso ma wyższe RSS w porównaniu do regresji z regularyzacją grzbietową (gorsze dopasowanie modelu),\n","- wiele współczynników jest zerowa (nawet dla niewielkich wielkości $\\alpha$)."]},{"cell_type":"markdown","metadata":{"id":"PjHLAhpSCcl6"},"source":["## Porównanie regularyzacji grzbietowej z regularyzacją Lasso\n","\n","### Ridge\n","- zawiera wszystkie (lub żadne) cechy w modelu, główną zaletą tej regularyzacji jest **ściągniecie współczynników** (ang. **shrinkage coefficient**),\n","- regresji grzbietowej używa się głowniej do **uniknięcia przeuczenia** modelu, ale z racji, że zawiera wszystkie zmienne z modelu nie jest użyteczny w przypadku wielowymiarowych danych (gdy liczbę predyktorów szacuje się milionach/miliardach -- zbyt duża złożoność obliczeniowa),\n","- zasadniczo działa dobrze nawet w obecności silnie **skorelowanych** cech -- uwzględnia wszystkie skorelowane zmienne w modelu, ale wielkość współczynników zależy od wielkości korelacji.\n","\n","\n","### Lasso \n","- regularyzacja Lasso poza **ściągniecie współczynników**, dokonuje również selekcji zmiennych\n","- regularyzacje Lasso często wykorzystuje się do **selekcji zmiennych** w przypadku do liczba cech jest rzędu milionów/miliardów\n","- wybiera dowolną cechę spośród cech silnie skorelowanych, współczynniki pozostałych cechy skorelowanych z wybraną zmienną redukuje do zera, ale wybrana zmienna zmienia się losowo wraz ze zmianą parametrów modelu -- podejście te działa gorzej niż regularyzacja grzbietowa"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}