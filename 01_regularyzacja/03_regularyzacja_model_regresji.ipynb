{"cells":[{"cell_type":"markdown","metadata":{"id":"20fO2Az4YB7c"},"source":["# Regularyzacja w modelu regresji - zbiór `Hitter`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r2wz2g5gYB7j"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","import statsmodels.api as sm\n","import statsmodels.formula.api as smf\n","from patsy import dmatrices\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from matplotlib.pylab import rcParams\n","rcParams['figure.figsize'] = 12, 10\n","\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge, RidgeCV\n","from sklearn.linear_model import Lasso, LassoCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","from sklearn.model_selection import GridSearchCV, cross_val_score , train_test_split\n","import sklearn.metrics as metrics\n","\n","from scipy import stats"]},{"cell_type":"markdown","metadata":{"id":"12X_llZTYB7l"},"source":["# Zadanie - `Hitter`"]},{"cell_type":"markdown","metadata":{"id":"j1wmvcKVYB7m"},"source":["Zbiór `Hitter` (pakiet `ISLR`) zawiera dane z **Major League Baseball** z sezonu 1986 i 1987, zawierają 322 obserwacje na temat głównych zawodników scharakteryzowanym na podstawie następujących zmiennych: \n","\n","``AtBat``\n","   Number of times at bat in 1986\n","\n","``Hits``\n","   Number of hits in 1986\n","\n","``HmRun``\n","   Number of home runs in 1986\n","\n","``Runs``\n","   Number of runs in 1986\n","\n","``RBI``\n","   Number of runs batted in in 1986\n","\n","``Walks``\n","   Number of walks in 1986\n","\n","``Years``\n","   Number of years in the major leagues\n","\n","``CAtBat``\n","   Number of times at bat during his career\n","\n","``CHits``\n","   Number of hits during his career\n","\n","``CHmRun``\n","   Number of home runs during his career\n","\n","``CRuns``\n","   Number of runs during his career\n","\n","``CRBI``\n","   Number of runs batted in during his career\n","\n","``CWalks``\n","   Number of walks during his career\n","\n","``League``\n","   A factor with levels ``A`` and ``N`` indicating player's league at\n","   the end of 1986\n","\n","``Division``\n","   A factor with levels ``E`` and ``W`` indicating player's division at\n","   the end of 1986\n","\n","``PutOuts``\n","   Number of put outs in 1986\n","\n","``Assists``\n","   Number of assists in 1986\n","\n","``Errors``\n","   Number of errors in 1986\n","\n","``Salary``\n","   1987 annual salary on opening day in thousands of dollars\n","\n","``NewLeague``\n","   A factor with levels ``A`` and ``N`` indicating player's league at\n","   the beginning of 1987"]},{"cell_type":"markdown","metadata":{"id":"6AZ-mb11YB7o"},"source":["1. Podziel zbiór losowo na część treningową i testową w stosunku 7:3\n","\n","2. Dopasuj model regresji liniowej na danych treningowych, w którym zmienną zależną jest zmienna `Salary` a pozostałe cechy zmiennymi niezależnymi.\n","Przy użyciu zbioru testowego określ własności predykcyjne modelu korzystając z  następujących miar:\n","   - błędu średniokwadratowego, \n","   - mediany błędu bezwzględnego.\n","    \n","    \n","3. Dopasuj na zbiorze treningowym model regresji grzbietowej:\n","\n","   a) dla dowolnie wybranego parametru $\\alpha$ określ, przy użyciu zbioru testowego, własności predykcyjne modelu korzystając z następujących miar:\n","   - błędu średniokwadratowego, \n","   - mediany błędu bezwzględnego.\n","   \n","   b) Korzystając z kroswalidacji na zb. treningowym znajdź optymalną wartość parametru $\\alpha$ (`GridSearchCV`)\n","   \n","   c) sporządź wykres wartości współczynników regresji względem parametru $\\alpha$.\n","\n","   d) dla optymalnego parametru $\\alpha$ określ, przy użyciu zbioru testowego,własności predykcyjne modelu korzystając z następujących miar:\n","   - błędu średniokwadratowego, \n","   - mediany błędu bezwzględnego.\n","   \n","\n","4. Dopasuj na zbiorze treningowym model regresji Lasso:\n","\n","   a) dla dowolnie wybranego parametru $\\alpha$ określ, przy użyciu zbioru testowego, własności predykcyjne modelu korzystając z następujących miar:\n","   - błędu średniokwadratowego, \n","   - mediany błędu bezwzględnego.\n","   \n","   b) Korzystając z kroswalidacji na zb. treningowym znajdź optymalną wartość parametru $\\alpha$ (`GridSearchCV`)\n","   \n","   c) sporządź wykres wartości współczynników regresji względem parametru $\\alpha$.\n","\n","   d) dla optymalnego parametru $\\alpha$ określ, przy użyciu zbioru testowego,własności predykcyjne modelu korzystając z następujących miar:\n","   - błędu średniokwadratowego, \n","   - mediany błędu bezwzględnego."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V7yt3yBOYB7p"},"outputs":[],"source":["# hitters = sm.datasets.get_rdataset(dataname=\"Hitters\", package=\"ISLR\", cache=True)\n","# # print(hitters.__doc__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zWgpzdUHYB7q"},"outputs":[],"source":["# hitters.data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"8ciPJf-TYB7s"},"outputs":[],"source":["# hitters.data.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dk_XTWRFYB7t"},"outputs":[],"source":["# hitters = hitters.data.dropna()\n","# hitters = hitters.drop(['League', 'Division', 'NewLeague'], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JVRvIB1TYB7u"},"outputs":[],"source":["# ## Podział zbioru na zmienne niezależne i zmienną zależną\n","# X, y = hitters.iloc[:, hitters.columns != 'Salary'], hitters['Salary']"]},{"cell_type":"code","source":["# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123) #podział na część treningową i testową"],"metadata":{"id":"TAMqPkrqyfFK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KoiuYqLUYB7v"},"outputs":[],"source":["# #zwykła regresja liniowa na zb. treningowym\n","# model_linear_regression = LinearRegression()\n","# model_linear_regression.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z4Kpm26vYB7x"},"outputs":[],"source":["# mse = metrics.mean_squared_error(y_true=y_test, y_pred=model_linear_regression.predict(X_test))\n","# mae = metrics.median_absolute_error(y_true=y_test, y_pred=model_linear_regression.predict(X_test))\n","\n","# results_linear_regression = {}\n","# results_linear_regression['mse'] = mse\n","# results_linear_regression['mae'] = mae"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bcMqGJCLYB7y"},"outputs":[],"source":["# results_linear_regression"]},{"cell_type":"code","source":["# # MSE o wiele większe od MAE może być to spowodowane bardzo niesymetrycznym rozkładem y\n","# plt.hist(y)"],"metadata":{"id":"MC84HqWpaLGk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4045THV1YB7z"},"outputs":[],"source":["# # Regresja z regularyzacja Ridge\n","# model_ridge_regression = make_pipeline(\n","#     StandardScaler(), \n","#     Ridge(alpha=1)\n","# )"]},{"cell_type":"code","source":["# model_ridge_regression.fit(X_train,y_train) #skalujemy zmienne i dopasowujemy model"],"metadata":{"id":"hw7Cgoni2A-C"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Xk_fw6lYB70"},"outputs":[],"source":["# #rozważamy miary MSE, MAE dla konkretnego modelu\n","# def pred_model(model, X, y):\n","#     mse = metrics.mean_squared_error(y_true=y, y_pred=model.predict(X))\n","#     mae = metrics.median_absolute_error(y_true=y, y_pred=model.predict(X))\n","\n","#     results = {}\n","#     results['mse'] = mse\n","#     results['mae'] = mae\n","#     return results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RxfsFsRFYB70"},"outputs":[],"source":["# #lepsza predykcja niż w przypadku zwykłej regresji liniowej\n","\n","# results_ridge_alpha_1 = pred_model(model_ridge_regression, X_test, y_test)\n","# print(results_ridge_alpha_1)"]},{"cell_type":"markdown","metadata":{"id":"JZ9kgvlRYB71"},"source":["# `Gridsearch`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQ8XtueYYB71"},"outputs":[],"source":["# #wcześniej patrzyliśmy jak się to zachowuje dla ustalonej alphy\n","# estimator_ridge = make_pipeline(\n","#     StandardScaler(), \n","#     Ridge()\n","# )\n","\n","# #grid bierze estymator i wyszukuje jak najlepszy jego parametr (w przypadku ridge czy lasso chodzi o parametr alpha)\n","# grid = GridSearchCV(\n","#     estimator=estimator_ridge, \n","#     param_grid = {'ridge__alpha': np.linspace(0.01, 15, 100)},\n","#     scoring='neg_mean_squared_error',\n","#     cv=10\n","# )\n","# #dopasowujemy model ridge dla alpha optymalnego dla danych na których uczymy model\n","# grid.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NDPDhNRtYB72"},"outputs":[],"source":["# grid.best_params_ #najlepsza alpha"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BsBNw6zEYB73"},"outputs":[],"source":["# -grid.best_score_ #MSE na zb. treningowym"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2l7l-npuYB74"},"outputs":[],"source":["# grid_best = grid.best_estimator_\n","\n","# # ridge = make_pipeline(\n","# #     StandardScaler(), \n","# #     Ridge(alpha=grid.best_params_)\n","# # ) - Ridge z najlepszą alphą"]},{"cell_type":"code","source":["# print(grid_best['ridge'].coef_)\n","# grid_best['ridge'].intercept_ #współczynniki"],"metadata":{"id":"mnbrm5Spfqnf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qu9dsK9YB74"},"outputs":[],"source":["# results_ridge_alpha_opt = pred_model(grid.best_estimator_, X_test, y_test) # jeszcze mniejsze.\n","# print(results_ridge_alpha_opt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7FnoAYQYB74"},"outputs":[],"source":["# #2c\n","# #patrzymy jak zmieniają się współczynniki beta ridge wraz ze wzrostem alphy \n","# plt.figure(figsize=(20, 10))\n","\n","# alpha_vec =  np.linspace(0, 100, 100)\n","\n","# coefs = []\n","# for a in alpha_vec:\n","#     model_ridge_regression = make_pipeline(\n","#         StandardScaler(),\n","#         Ridge(alpha = a)\n","#     )\n","#     model_ridge_regression.fit(X_train, y_train)\n","#     coefs.append(model_ridge_regression.named_steps['ridge'].coef_)\n","\n","\n","# plt.plot(alpha_vec, coefs)\n","# plt.plot([np.min(alpha_vec), np.max(alpha_vec)], [0, 0], '-.', color = 'black')\n","# plt.title('Wykres wielkości współczynników regresji liniowej z regularyzacją grzbietową w zależności od siły regularyzacji (parametr alpha)')\n","# plt.xlabel('alpha')\n","# plt.ylabel('coefficients')\n","# plt.legend(X.columns)\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"fiKEcORsYB75"},"source":["## Regresja wielomianowa z regularyzacją ridge"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"EBfx6afiYB75"},"outputs":[],"source":["# #teraz łączymy regresję wielomianową z regularyzacją ridge\n","# poly_ridge_estimator = make_pipeline(\n","#     PolynomialFeatures(include_bias = False),\n","#     StandardScaler(),\n","#     Ridge()\n","# )\n","# poly_ridge_estimator"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"wTevmFGQYB76"},"outputs":[],"source":["# #bierzemy siatkę potęg wielomianów i siatkę alph\n","# grid2 = GridSearchCV(\n","#     estimator=poly_ridge_estimator,\n","#     param_grid={\n","#         'polynomialfeatures__degree': [1, 2, 3],\n","#         'ridge__alpha': np.linspace(5, 15, 20)\n","#     },\n","#     scoring='neg_mean_squared_error',\n","#     cv=10\n","# )\n","\n","# grid2.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zS-aGykNYB76"},"outputs":[],"source":["# grid2.best_params_ #optymalne parametry"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7HkUIoZyYB76"},"outputs":[],"source":["# results_poly_ridge_alpha_opt = pred_model(grid2.best_estimator_, X_test, y_test) #miary dają większy nieco wynik na MSE, niż metode bez dodania PolynomialFeatures, ale zdecydowanie mniejszy na MAE\n","# print(results_poly_ridge_alpha_opt)"]},{"cell_type":"markdown","metadata":{"id":"AzId_gSEYB76"},"source":["## Lasso"]},{"cell_type":"markdown","source":["## Regresja wielomianowa z regularyzacją lasso"],"metadata":{"id":"u3JLNMQNLfGh"}},{"cell_type":"code","source":["# #wyniki końcowe\n","# MSES =np.array([[results_linear_regression[\"mse\"],\n","#        results_ridge_alpha_1[\"mse\"] ,results_ridge_alpha_opt[\"mse\"],results_poly_ridge_alpha_opt[\"mse\"],\n","#        results_lasso_alpha_1[\"mse\"] ,results_lasso_alpha_opt[\"mse\"],results_poly_lasso_alpha_opt[\"mse\"]]])\n","# MAES =np.array([[results_linear_regression[\"mae\"],\n","#        results_ridge_alpha_1[\"mae\"] ,results_ridge_alpha_opt[\"mae\"],results_poly_ridge_alpha_opt[\"mae\"],\n","#        results_lasso_alpha_1[\"mae\"] ,results_lasso_alpha_opt[\"mae\"],results_poly_lasso_alpha_opt[\"mae\"]]])\n"],"metadata":{"id":"X_RPlyy_BSLM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pd.DataFrame(np.concatenate((MSES, MAES),axis = 0),columns=['linear', 'ridge_alpha=1', 'ridge_alpha_opt','ridge_poly_alpha_opt',\n","#                                                             'lasso_alpha=1', 'lasso_alpha_opt','lasso_poly_alpha_opt'],\n","#              index = [\"MSE\",\"MAE\"])"],"metadata":{"id":"FfUY59rSFQaV"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}